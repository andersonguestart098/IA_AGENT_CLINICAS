import os
import traceback
import logging
import sys
import io
from functools import lru_cache
from transformers import AutoTokenizer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from fastapi import FastAPI, UploadFile, File, HTTPException, Form
from fastapi.responses import HTMLResponse
from pydantic import BaseModel
from llama_cpp import Llama
from prisma import Prisma
from datetime import datetime
from pathlib import Path
import pytesseract
from PIL import Image
from io import BytesIO
import mimetypes
import pdfplumber
from pdf2image import convert_from_bytes
from dotenv import load_dotenv
import json
import asyncio
from models.embeddings import gerar_embedding

# Carregar vari√°veis de ambiente
load_dotenv()

# Configurar o Prisma
prisma = Prisma(auto_register=True)

# Configurar o DATABASE_URL diretamente (ajuste o caminho se necess√°rio)
os.environ["DATABASE_URL"] = "file:D:/IA2/CEMEAR-IA-FIX/prisma/dev.db"

# Criar diret√≥rios
Path("uploads").mkdir(parents=True, exist_ok=True)
Path("logs").mkdir(parents=True, exist_ok=True)

# Configurar o Tesseract (Windows)
pytesseract.pytesseract.tesseract_cmd = r"C:\\Program Files\\Tesseract-OCR\\tesseract.exe"

# Configurar logging com UTF-8
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler("logs/cemear.log", encoding="utf-8"),
        logging.StreamHandler(io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8"))
    ]
)
logger = logging.getLogger(__name__)

# Inicializar o FastAPI
app = FastAPI()

# Inicializar o LLM
llm = Llama(
    model_path="D:/huggingface_cache/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf",
    n_ctx=4096,
    n_gpu_layers=100,
    n_threads=16,
    n_batch=512,
    use_mlock=True,
    f16_kv=True,
    verbose=True
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Fun√ß√£o para extrair texto de arquivos
async def extrair_texto(content: bytes, file: UploadFile) -> str:
    texto = ""
    file_ext = Path(file.filename).suffix.lower()
    content_type = file.content_type or mimetypes.guess_type(file.filename)[0] or ""

    if content_type.startswith("image/") or file_ext in [".png", ".jpg", ".jpeg"]:
        image = Image.open(BytesIO(content))
        texto = pytesseract.image_to_string(image)
    elif content_type == "application/pdf" or file_ext == ".pdf":
        with pdfplumber.open(BytesIO(content)) as pdf:
            for page in pdf.pages:
                texto += page.extract_text() or ""
        if not texto.strip():
            images = convert_from_bytes(content)
            for image in images:
                texto += pytesseract.image_to_string(image)
    return texto

# Fun√ß√£o para truncar contexto com base em tokens
def truncate_context(documents, max_tokens=1500):
    context = ""
    current_tokens = 0
    for doc in documents:
        doc_text = f"- {doc}\n\n"
        doc_tokens = len(tokenizer.encode(doc_text))
        if current_tokens + doc_tokens > max_tokens:
            remaining_tokens = max_tokens - current_tokens
            truncated_doc = tokenizer.decode(tokenizer.encode(doc_text)[:remaining_tokens])
            context += truncated_doc
            break
        context += doc_text
        current_tokens += doc_tokens
    return context.strip()

# Cache para gera√ß√£o de embeddings
@lru_cache(maxsize=1000)
def cached_gerar_embedding(question: str) -> str:
    return gerar_embedding(question)

# Eventos de inicializa√ß√£o e encerramento
@app.on_event("startup")
async def startup():
    await prisma.connect()
    logger.info("‚úÖ Prisma conectado com sucesso.")
    try:
        logger.info(f"üìÇ Banco em uso: {os.environ['DATABASE_URL']}")
    except Exception:
        logger.warning("‚ö†Ô∏è N√£o foi poss√≠vel identificar o caminho do banco.")

@app.on_event("shutdown")
async def shutdown():
    await prisma.disconnect()
    logger.info("üîï Prisma desconectado.")

# Modelo para requisi√ß√µes de chat
class PromptRequest(BaseModel):
    question: str

# Endpoint de chat
@app.post("/chat")
async def chat_endpoint(data: PromptRequest):
    try:
        if not data.question or len(data.question.strip()) < 3:
            logger.warning("‚ö†Ô∏è Pergunta inv√°lida recebida.")
            return {"answer": "Por favor, envie uma pergunta v√°lida.\nFim da resposta."}

        logger.info(f"ü§ñ Pergunta recebida no /chat: {data.question}")

        # Buscar todos os documentos com embeddings
        for attempt in range(3):
            try:
                todos = await prisma.knowledgebase.find_many(where={"embedding": {"not": None}})
                break
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Tentativa {attempt + 1} falhou: {e}")
                if attempt == 2:
                    raise HTTPException(status_code=500, detail="Erro ao acessar o banco de dados.")
                await asyncio.sleep(1)

        if not todos:
            logger.info("‚ö†Ô∏è Nenhum documento com embedding encontrado.")
            return {"answer": "N√£o encontrei essa informa√ß√£o nos documentos t√©cnicos.\nFim da resposta."}

        # Gerar embedding da pergunta
        embedding_pergunta = np.array(json.loads(cached_gerar_embedding(data.question))).reshape(1, -1)

        embeddings = []
        metadata = []
        for k in todos:
            try:
                vetor = np.array(json.loads(k.embedding)).reshape(1, -1)
                embeddings.append(vetor)
                metadata.append(k)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro ao processar embedding ID {k.id}: {e}")
                continue

        if not embeddings or not metadata:
            logger.warning("‚ö†Ô∏è Nenhum embedding utiliz√°vel.")
            return {"answer": "N√£o encontrei essa informa√ß√£o nos documentos t√©cnicos.\nFim da resposta."}

        # Similaridade entre pergunta e base
        embeddings_stack = np.vstack(embeddings)
        scores = cosine_similarity(embedding_pergunta, embeddings_stack)[0]

        # Ordenar por similaridade
        relevantes = [(metadata[i], score) for i, score in enumerate(scores)]
        top_relevantes = sorted(relevantes, key=lambda x: x[1], reverse=True)
        top_validos = [r for r in top_relevantes if r[1] >= 0.4][:5]  # Limiar mais tolerante

        if not top_validos:
            logger.info("‚ö†Ô∏è Nenhuma similaridade relevante encontrada.")
            return {"answer": "N√£o encontrei essa informa√ß√£o nos documentos t√©cnicos.\nFim da resposta."}

        # Log de documentos usados
        for k, score in top_validos:
            logger.info(f"üß© ID: {k.id} | Score: {round(score, 4)} | Origem: {k.origem}")

        # Constru√ß√£o do prompt baseado nos melhores documentos
        documentos = '\n'.join(f"- (Score: {round(score, 4)}) {k.conteudo.strip()}" for k, score in top_validos)

        prompt = f"""
Voc√™ √© um assistente t√©cnico da empresa Cemear.

Responda apenas com base nos documentos t√©cnicos fornecidos abaixo.  
Priorize informa√ß√µes de documentos com maior pontua√ß√£o de similaridade.  
Se os documentos n√£o responderem completamente √† pergunta, diga:  
"N√£o encontrei essa informa√ß√£o nos documentos t√©cnicos."  
Finalize sempre com: "Fim da resposta."

üìö Documentos t√©cnicos relevantes:
{documentos}

‚ùì Pergunta:
{data.question}

üìù Resposta:
"""

        resposta_raw = llm(
            prompt,
            max_tokens=1024,
            temperature=0.1,
            top_p=0.6,
            repeat_penalty=1.8,
            presence_penalty=1.0
        )["choices"][0]["text"].strip()

        resposta_final = resposta_raw
        if "Fim da resposta" not in resposta_final:
            resposta_final += "\nFim da resposta."

        if len(resposta_final.strip()) < 10:
            resposta_final = "N√£o encontrei essa informa√ß√£o nos documentos t√©cnicos.\nFim da resposta."

        logger.info(f"üß† Resposta final: {resposta_final}")
        return {"answer": resposta_final}

    except Exception:
        logger.error("Erro no LLM: %s", traceback.format_exc())
        raise HTTPException(status_code=500, detail="Erro ao processar a pergunta.")



# Endpoint para upload de conhecimento
@app.post("/upload")
async def upload_conhecimento(file: UploadFile = File(...)):
    try:
        content = await file.read()
        file_ext = Path(file.filename).suffix.lower()
        texto = content.decode("utf-8", errors="ignore") if file_ext == ".txt" else await extrair_texto(content, file)

        if not texto.strip():
            raise HTTPException(status_code=422, detail="N√£o foi poss√≠vel extrair texto do arquivo.")

        embedding_json = gerar_embedding(texto)  # <- string JSON com o vetor

        kb = await prisma.knowledgebase.create({
            "origem": file.filename,
            "conteudo": texto,
            "embedding": embedding_json
        })

        return {"status": "sucesso", "resumo": texto[:200], "id": kb.id}
    except Exception:
        logger.error("Erro ao salvar conhecimento: %s", traceback.format_exc())
        raise HTTPException(status_code=500, detail="Erro interno ao processar o upload.")


# Endpoint para upload de planta
@app.post("/upload-planta")
async def upload_planta(file: UploadFile = File(...), contexto: str = Form("")):
    try:
        content = await file.read()
        texto = await extrair_texto(content, file)
        if not texto.strip():
            raise HTTPException(status_code=422, detail="N√£o foi poss√≠vel extrair texto da planta.")

        largura, altura = 5.0, 3.0  # Valores fixos como exemplo
        area = largura * altura
        materiais = {
            "area_total_m2": round(area, 2),
            "perimetro_total_m": round(2 * (largura + altura), 2),
            "montantes": int(area * 2),
            "guias": int(largura * 2),
            "placas_gesso": int(area / 1.8),
            "parafusos": int(area * 15),
            "fitas": int(area / 50),
            "massa": int(area / 20)
        }

        prompt = f"""
Voc√™ √© um engenheiro da Cemear.

Contexto:
{contexto}

Texto da planta:
{texto[:1200]}...

Gere uma resposta t√©cnica com materiais estimados.
"""
        resposta = llm(prompt, max_tokens=1024, temperature=0.7, top_p=0.9)["choices"][0]["text"].strip()
        kb = await prisma.knowledgebase.create({"origem": file.filename, "conteudo": texto})

        return {
            "status": "sucesso",
            "materiais_estimados": materiais,
            "medidas_detectadas": {"largura_metros": largura, "altura_metros": altura},
            "resposta_ia": resposta,
            "resumo": texto[:200],
            "knowledgeBaseId": kb.id
        }
    except Exception:
        logger.error("Erro ao processar planta: %s", traceback.format_exc())
        raise HTTPException(status_code=500, detail="Erro interno ao processar a planta.")

# Endpoint para interpreta√ß√£o de upload
@app.post("/upload-interpreta")
async def upload_interpreta(file: UploadFile = File(...), contexto: str = Form("")):
    try:
        content = await file.read()
        texto = await extrair_texto(content, file)
        if not texto.strip() and not contexto.strip():
            raise HTTPException(status_code=422, detail="N√£o foi poss√≠vel interpretar nada.")

        prompt = (
            f"O usu√°rio disse: {contexto}\nArquivo: {texto[:1200]}" 
            if texto and contexto 
            else (f"Conte√∫do do arquivo: {texto[:1200]}" if texto else f"Contexto do usu√°rio: {contexto}")
        )
        resposta = llm(prompt, max_tokens=1024, temperature=0.7, top_p=0.9)["choices"][0]["text"].strip()

        return {"status": "sucesso", "resposta_ia": resposta, "resumo": texto[:200] if texto else contexto[:200]}
    except Exception:
        logger.error("Erro ao interpretar: %s", traceback.format_exc())
        raise HTTPException(status_code=500, detail="Erro interno ao interpretar.")
    
@app.post("/calcular-materiais")
async def calcular_materiais_manualmente(
    area: float = Form(...),
    perimetro: float = Form(...),
    altura_rebaixo: float = Form(0.4),
    contexto: str = Form("")
):
    from math import ceil

    try:
        logger.info(f"üìê C√°lculo manual solicitado - √Årea: {area}, Per√≠metro: {perimetro}, Rebaixo: {altura_rebaixo}")
        logger.info(f"üìù Contexto: {contexto}")

        # C√°lculos t√©cnicos
        f530 = (area * 1.8) / 3
        massa = area * 0.55
        fita = (area * 1.5) / 150
        gn25 = area * 16
        cantoneira = (perimetro * 1.05) / 3
        pendural = f530 * 3
        arame10 = (pendural * altura_rebaixo) / 14
        parafuso_bucha = (cantoneira * 6) + pendural
        parafuso_13mm_pa = f530 * 2

        # Resposta com base no contexto
        contexto_lower = contexto.lower()

        if "f530" in contexto_lower or "montante" in contexto_lower:
            resposta = f"Com base na √°rea e per√≠metro fornecidos, a quantidade estimada de montantes F530 √© de {ceil(f530)} barras de 3 metros."
        elif "massa" in contexto_lower:
            resposta = f"Com base na √°rea fornecida, a quantidade estimada de massa √© de {massa:.2f} kg."
        elif "fita" in contexto_lower:
            resposta = f"Com base na √°rea fornecida, a quantidade estimada de fita de papel √© de {ceil(fita)} rolos."
        elif "gn25" in contexto_lower:
            resposta = f"Com base na √°rea fornecida, a quantidade estimada de parafusos GN25 √© de {int(gn25)} unidades."
        elif "cantoneira" in contexto_lower:
            resposta = f"Com base no per√≠metro fornecido, a quantidade estimada de cantoneiras tabica √© de {ceil(cantoneira)} barras de 3 metros."
        elif "pendural" in contexto_lower:
            resposta = f"Com base na √°rea fornecida, a quantidade estimada de pendurais √© de {ceil(pendural)} unidades."
        elif "arame" in contexto_lower:
            resposta = f"Com base na √°rea e altura do rebaixo, a quantidade estimada de arame 10 √© de {arame10:.2f} kg."
        elif "bucha" in contexto_lower:
            resposta = f"Com base nos c√°lculos, a quantidade estimada de parafusos com bucha √© de {ceil(parafuso_bucha)} unidades."
        elif "parafuso 13mm" in contexto_lower or "pa" in contexto_lower:
            resposta = f"Com base na √°rea fornecida, a quantidade estimada de parafusos 13mm PA √© de {ceil(parafuso_13mm_pa)} unidades."
        else:
            resposta = "N√£o encontrei essa informa√ß√£o nos documentos t√©cnicos."

        return {
            "status": "sucesso",
            "resposta_ia": resposta
        }

    except Exception as e:
        logger.error(f"Erro no c√°lculo manual com RAG: {str(e)}")
        raise HTTPException(status_code=500, detail="Erro ao calcular com base no banco de conhecimento.")



# Modelo para requisi√ß√µes de feedback
class FeedbackRequest(BaseModel):
    question: str
    answer: str
    feedback: str
    contextoUsuario: str = ""
    origemPlanta: str = ""
    knowledgeBaseId: int | None = None

# Endpoint para receber feedback
@app.post("/feedback")
async def receber_feedback(data: FeedbackRequest):
    try:
        # Prompt para classificar feedback com LLaMA
        prompt_classificacao = f"""
Classifique o feedback a seguir com base nas regras:

Feedback do usu√°rio:
"{data.feedback}"

Regras:
- Se o feedback indicar que a resposta foi correta, diga apenas: ACERTO
- Se o feedback indicar erro ou insatisfa√ß√£o, diga apenas: ERRO
- Se for neutro, elogio gen√©rico ou n√£o der pra saber, diga apenas: NEUTRO

IMPORTANTE: Retorne SOMENTE uma destas palavras em MAI√öSCULO: ACERTO, ERRO ou NEUTRO. Sem explica√ß√µes, sem pontua√ß√£o, sem prefixo.
"""

        # Chamada ao LLaMA com temperatura 0 (determin√≠stico)
        classificacao_raw = llm(
            prompt_classificacao,
            max_tokens=5,
            temperature=0.0
        )["choices"][0]["text"].strip().upper()

        # Sanitiza√ß√£o da sa√≠da para detectar a inten√ß√£o correta
        if "ACERTO" in classificacao_raw:
            acerto = True
            classificacao_final = "ACERTO"
        elif "ERRO" in classificacao_raw:
            acerto = False
            classificacao_final = "ERRO"
        else:
            acerto = False
            classificacao_final = "NEUTRO"

        # Cria√ß√£o do feedback no banco
        created = await prisma.feedback.create({
            "question": data.question,
            "answer": data.answer,
            "feedback": data.feedback,
            "acerto": acerto,
            "contextoUsuario": data.contextoUsuario,
            "origemPlanta": data.origemPlanta,
            "knowledgeBaseId": data.knowledgeBaseId
        })

        logger.info(f"üì• Feedback salvo: ID {created.id} | Classifica√ß√£o: {classificacao_final}")
        return {"status": "ok", "id": created.id, "classificacao": classificacao_final}

    except Exception as e:
        logger.error(f"Erro ao salvar feedback: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erro ao salvar feedback: {str(e)}")



# Endpoint para aprendizado por refor√ßo
@app.post("/reforco")
async def aprendizado_reforco():
    try:
        feedbacks = await prisma.feedback.find_many(where={"acerto": True, "usada_para_treinamento": False})
        contador = 0

        for fb in feedbacks:
            content = f"Pergunta: {fb.question}\nResposta: {fb.answer}"
            try:
                embedding_json = gerar_embedding(content)  # gera vetor como string JSON
                kb = await prisma.knowledgebase.create({
                    "origem": f"feedback:{fb.id}",
                    "conteudo": content,
                    "embedding": embedding_json
                })
                await prisma.feedback.update(
                    where={"id": fb.id},
                    data={"usada_para_treinamento": True, "knowledgeBaseId": kb.id}
                )
                contador += 1
            except Exception as e:
                logger.error(f"Erro ao vetorizar feedback {fb.id}: {str(e)}")
                continue

        logger.info(f"Refor√ßo aplicado em {contador} feedbacks.")
        return {"status": "refor√ßo aplicado", "quantidade": contador}
    except Exception:
        logger.error("Erro no refor√ßo: %s", traceback.format_exc())
        raise HTTPException(status_code=500, detail="Erro ao aplicar refor√ßo.")

# Endpoint para testar feedbacks
@app.get("/test-feedback")
async def test_feedback():
    try:
        feedbacks = await prisma.feedback.find_many(
            order={"contextoUsuario": "asc"},
            take=100,
            skip=0,
            include={"knowledgeBase": True}
        )
        return {
            "status": "success",
            "data": [
                {
                    "id": f.id,
                    "question": f.question,
                    "answer": f.answer,
                    "feedback": f.feedback,
                    "acerto": f.acerto,
                    "contextoUsuario": f.contextoUsuario,
                    "origemPlanta": f.origemPlanta,
                    "knowledgeBase": f.knowledgeBase.conteudo if f.knowledgeBase else None
                }
                for f in feedbacks
            ]
        }
    except Exception as e:
        logger.error(f"Erro ao buscar feedbacks: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erro ao buscar feedbacks: {str(e)}")
    
@app.get("/metrics")
async def gerar_metricas():
    try:
        total = await prisma.feedback.count()
        acertos = await prisma.feedback.count(where={"acerto": True})
        usados = await prisma.feedback.count(where={"usada_para_treinamento": True})

        taxa = round((acertos / total) * 100, 2) if total else 0
        usado_pct = round((usados / total) * 100, 2) if total else 0

        ultima = await prisma.metricas.find_first(order={"criadoEm": "desc"})

        # Calcula a evolu√ß√£o se houver registro anterior
        evolucao_taxa = round(taxa - ultima.taxaAcerto, 2) if ultima else None
        evolucao_usados = round(usado_pct - ultima.percentualUsado, 2) if ultima else None

        # Salva nova m√©trica
        await prisma.metricas.create({
            "totalFeedbacks": total,
            "acertos": acertos,
            "taxaAcerto": taxa,
            "usadosTreino": usados,
            "percentualUsado": usado_pct
        })

        return {
            "total_feedbacks": total,
            "acertos": acertos,
            "taxa_acerto": taxa,
            "feedbacks_usados": usados,
            "percentual_usado": usado_pct,
            "evolucao_taxa_acerto": evolucao_taxa,
            "evolucao_percentual_usado": evolucao_usados
        }

    except Exception:
        logger.error("Erro ao gerar m√©tricas: %s", traceback.format_exc())
        raise HTTPException(status_code=500, detail="Erro ao gerar m√©tricas.")
    
@app.get("/dashboard", response_class=HTMLResponse)
async def exibir_dashboard():
    total = await prisma.feedback.count()
    acertos = await prisma.feedback.count(where={"acerto": True})
    usados = await prisma.feedback.count(where={"usada_para_treinamento": True})

    taxa_acerto = round((acertos / total) * 100, 2) if total else 0
    percentual_usado = round((usados / total) * 100, 2) if total else 0

    html = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <title>Dashboard IA - Cemear</title>
        <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
    </head>
    <body class="bg-gray-100 text-gray-800">
        <div class="container mx-auto p-8">
            <h1 class="text-3xl font-bold mb-4">üìä Dashboard da IA - Cemear</h1>
            <div class="grid grid-cols-2 gap-4">
                <div class="bg-white shadow-md rounded-lg p-6">
                    <h2 class="text-lg font-semibold">Feedbacks totais</h2>
                    <p class="text-2xl">{total}</p>
                </div>
                <div class="bg-white shadow-md rounded-lg p-6">
                    <h2 class="text-lg font-semibold">Feedbacks com acerto</h2>
                    <p class="text-2xl">{acertos} ({taxa_acerto}%)</p>
                </div>
                <div class="bg-white shadow-md rounded-lg p-6">
                    <h2 class="text-lg font-semibold">Usados para treinamento</h2>
                    <p class="text-2xl">{usados} ({percentual_usado}%)</p>
                </div>
            </div>
        </div>
    </body>
    </html>
    """
    return HTMLResponse(content=html)


# Executar a aplica√ß√£o
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)